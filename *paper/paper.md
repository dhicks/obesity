---
title: Model-Based Science and Statistical Modeling
author: Daniel J. Hicks and Catherine Womack
---

# Introduction #

# An "Obesity Paradox"? #

# Statistical Models as Inferential Tools #

Many philosophers of science may assume a sharp distinction between "models of phenomena" — models as discussed in the model-based-science literature — and the models constructed in the practice of inferential statistics.  The latter statistical models may be regarded as "models of data," which merely "correct[], rectif[y], [and] regiment[]" the raw data, then "present the data in a 'neat' way, for instance by drawing a smooth curve through a set of points" *[https://plato.stanford.edu/entries/models-science/#SemModRep].  The philosophically interesting questions about models of data, in this sense, might seem to be exhausted by discussions of trimming outliers and the role of simplicity in curve fitting *[https://plato.stanford.edu/entries/statistics/#StaMod].  

However, this account of statistical models as models of data — especially in an epistemically derogatory sense, as *mere* models of data — seems to overlook two major features of their use in scientific practice.  First, statistical models are used for inductive and abductive inference:  to make predictions about future observations and to draw inferences about the values of (in some sense) unobservable variables.[^parameters]  Epistemically, this ampliative use of statistical models goes well beyond a neat presentation of data.  Second, classical statisticians (here including the few philosophers of statistics who have looked closely at actual statistical practice, but not including the machine learning community [@Breiman??, @Donoho??]) regard statistical models as *models of the data-generation process* [@Suppes1962; @Mayo??; @Huebner??].  As Wasserman puts it, "Statistical inference ... is the process of using data to *infer the distribution that generated the data*. A typical statistical inference question is: Given a sample $X_1, \ldots, X_n \sim F$, how do we infer $F$" [@Wasserman:AllOf, 87, my emphasis; compare @Mayo??, 382, 384]?  (Here $\sim$ should be read "is sampled from" or equivalently "is generated by.")  Given that this generating distribution cannot be observed directly — all we have are the finite number of noisy observations in the sample — this second feature is closely connected to the abductive use of statistical models.[^neat] 

[^parameters]: Specifically, statistical models are generally used abductively to make inferences about the "true" values of population parameters; for example, the population mean, as distinct from the mean observed in the available sample, or indeed any possible sample.  These parameters are unobservable in at least two senses.  First, any observations we make will be of a sample, finite, and influenced by measurement errors, limited precision, and other uncertainties.  We never directly observe the population itself, as it were.  Second, and more fundamentally, the population will often be counterfactual, such as an infinite population of US residents aged 18-84 in 2005.  While it's not clear to us that statistical populations are unobservable in the same sense that electrons are standardly taken to be unobservable [@vF:Scientific, 77], it does seem reasonable to characterize inferences about them as "abductive."

[^neat]: In the history of statistics, the view of statistical models as mere "neat" presentations of raw data seems to be associated with Karl Pearson, who viewed the aim of science as "the discovery ... of a brief statement or formula, which in a few words resumes a wide range of facts" [@Pearson:Grammar, ch. III §1].  However, this positivist interpretation of statistics was not shared with his contemporaries [@Pence:EarlyHistory, [8-9]] or intellectual (and biological) descendants [@Hodge??, especially 242; @Mayo??, ch. 11, esp. 380ff].}  

We believe that — while there are still important and interesting differences between statistical models and the "models of phenomena" examined in the model-based-science literature — this literature provides some conceptual tools that are useful for analyzing statistical models and their use in scientific practice.  In the remainder of this section, we first discuss the variety of fidelity criteria used to evaluate statistical models, then different notions of model uncertainty.  

## Fidelity Criteria for Statistical Models ##

Specifically, given the two features identified above, in @Weisberg:Simulation's terminology, statistical models are \technical{target-directed}:  they are intended to "generate predictions and explanations about" their target systems [@Weisberg:Simulation, 74].  Successfully generating such predictions and explanations requires that the model satisfy some \technical{fidelity criteria}, which "describe how similar the model must be to the [target] in order to be considered an adequate representation [of the target]" [@Weisberg:Simulation, 41].  Weisberg distinguishes two types of such fidelity criteria, which in the context of statistical models we will call \technical{generative} and \technical{predictive} ("representative" and "dynamical," respectively, in Weisberg's terminology).  

### Predictive Fidelity Criteria ###

Predictive criteria evaluate a model in terms of the accuracy of its predictions about the target system.  For example, ordinary least squares regression [OLS] fits a line $\hat Y = X\beta$ (where here $X$ is a $n \times p$ matrix of $n$ observations of $p$ covariates, and $\beta$ is a vector of $p$ coefficients) that minimizes the error statistic $RSS = \sum_{i=1}^N (y_i - \hat y_i)^2$, that is, the sum of the square errors between the predicted values $\hat y_i = x_i\beta$ and the observed or true values $y_i$ (where $x_i$ is the $i$th row of $X$). In the OLS case, minimizing this error statistic is equivalent to maximizing the log likelihood of the regression parameters $\beta$: $L(\beta) = \sum_{i=1}^N \log p(y_i | x_i\beta)$ [@Hastie:Elements 30-1].  In more general statistical modeling contexts — such as generalized linear regression, discussed in *[xref]* — this in turn is equivalent to minimizing the \technical{deviance} of the parameters $\theta$, defined simply as $-2L(\theta)$.  Deviance therefore is one generalization of the notion of error in OLS, and maximum likelihood methods can be understood in terms of fitting a model by maximizing its accuracy.  

Often we are most interested in \technical{out-of-sample} predictions, that is, predictive accuracy on a set of cases that aren't included in the original model.  When the total dataset is large enough, this can be done directly, by dividing the data into mutually exclusive \technical{training} and \technical{testing} sets.  

The \technical{Akaike Information Criterion} [AIC] — and related measures such as the so-called Bayesian Information Criterion [BIC] — estimate out-of-sample deviance by adding a penalty to in-sample deviance [@Hastie2009, 230; @James2013, 210ff].  However, AIC can only be used to compare models fit using exactly the same data.  This can be seen simply by noting that the log likelihood is not normalized by the sample size; so ceteris paribus a model fit with a larger dataset will have a larger deviance.  Further, models with different specifications of the response variable cannot necessarily be compared using AIC:  because divergence is based on the probability of observing the response data $y$, it matters how the response is characterized mathematically.  

Other predictive criteria are also used in statistical modeling, corresponding to other ways of operationalizing accuracy or error.  In many cases the response variable will be a binary positive or negative result, rather than a continuous value.  For example, in developing a diagnostic test for disease, the response may be that the patient or subject has the disease ("positive") or does not ("negative").  In this context, \technical{accuracy} is formally defined as the correct prediction rate ($p(\hat y = y)$); \technical{recall}, sensitivity, or the true positive rate is the probability that the model correctly predicts positive values ($p(\hat y = + | y = +)$); the false positive rate [FPR] is the probability that the model misses positive values ($p(\hat y = - | y = +)$); \technical{precision} or positive predictive value is the probability that the model's positive predictions are correct ($p(y = + | \hat y = +)$); and $F_1$ is the harmonic mean of precision and recall:  
$$ F_1 = 2 \frac{1}{\frac{1}{\textrm{precision}} + \frac{1}{\textrm{recall}}}. $$
Note that recall, precision, and therefore $F_1$ emphasize positive results, not negative ones.  For inductive risk reasons, these statistics will be less relevant in situations where false negatives are more important than false positives [@Hicks2018].  

In the binary response case, the immediate predictions of models will generally be continuous values, say $\hat z \in (0,1)$.  Mapping these continuous values to the binary result, $\hat z \to \hat y \in \{-, +\}$, generally involves comparing $\hat z$ to a \technical{discrimination threshold} parameter $\alpha$:  if $\hat z > \alpha$ then $\hat y = +$; otherwise $\hat y = -$.  The value of $\alpha$ is generally free, meaning that there are little or no a priori constraints on how it should be set.  Since different values of $\alpha$ give different predictions, the accuracy and error statistics discussed in the last paragraph can take different values depending on $\alpha$.  The \technical{receiver operator characteristic} or ROC curve is defined as the curve $f(\alpha) = (\textrm{recall}_\alpha, \textrm{FPR}_\alpha)$.  The area under this curve, abbreviated AUROC or AUC, is frequently used as a threshold-independent accuracy statistic.  AUROC values range from 0 to 1; 0.5 corresponds to a "naïve" or "random" predictor, such as flipping a coin with bias $\alpha$.  AUROC values reportedly estimate the probability that an arbitrary positive will have a higher prediction score than an arbitrary negative *[http://pubs.rsna.org/doi/pdf/10.1148/radiology.143.1.7063747], that is, $p(\hat z_2 > \hat z_1 | y_2 = +, y_1 = -)$.  

## Generative Fidelity Criteria ##

Above, we noted that statisticians generally regard statistical models as models of the data-generating process.  For example, the sampling process is enormously important to many statisticians.  The basic suite of models taught in introductory statistics classes assume simple random sampling.  Under a range of conditions, these model can still produce unbiased point estimates for non-random or more complex sampling processes; but the standard errors (estimates of parameter uncertainty) will generally be incorrect, leading to incorrect abductive inferences about the populations from which the samples were drawn.  Complex survey designs and analysis techniques can be used to produce correct standard errors, and therefore better abductive inferences [@Lumley:Complex, 1-6].  In short, different data-generating processes (simple random sampling vs.~complex surveys) require different statistical models.  

Concern with representing the data-generating process corresponds to Weisberg's second kind of fidelity criteria, which we call generative criteria.[^causal]  In statistical modeling practice, generative criteria are often operationalized in terms of concerns about \technical{bias}.  Formally, an estimate $\hat \theta$ of a parameter $\theta$ is \technical{unbiased} if the expected value $E[\hat \theta] = \theta$; that is (to use the frequentist interpretation), as we repeat the process of gathering data and calculating the estimate $\hat \theta$ indefinitely, the average of these estimates will converge to the true value $\theta$.  Failures to accurately represent the data-generating process tend to produce bias.  For example, incorrect assumptions about the sampling process — say, incorrectly assuming that that the sampling process is independent of some variable of interest — can lead to parameter estimates that will not tend to converge to the true population value.  This is called \technical{sampling bias}.  Similarly, failing to include substantial causal factors in a regression model can lead to biased estimates of the regression coefficients, called \technical{omitted-variable bias}.  Because "bias" refers to the expected value of a model's estimate, rather than the particular estimate that actually occurs, the term properly applies to a modeling *process* rather than a particular occurrent model.  A particular occurrent model may happen to have a perfect estimate even if it produced using a biased process, and vice versa [compare @Sober2015, 132]. 

[^causal]: The language of a "data-generating process" lends itself to misleadingly simple causal inferences.  For example, in a linear model $y = \beta_0 + \beta_1 x_1 + \beta_2 x_2$ with two predictors $x_1$ and $x_2$, if this model is read as a representation of the data-generating process, then it seems to follow that $x_1$ and $x_2$ are causes of $y$.  But of course $y$ might be a cause of $x_1$, and not vice versa; and $x_2$ might merely be correlated with some other variable $x'_2$ that is an actual cause of $y$ but is also much more difficult to measure than $x_2$.  We suggest that, in good cases of actual statistical practice, scientists draw on implicit or explicit background assumptions to select predictors that are causally upstream of the response (thus attempting to prevent the problem with $x_1$) and make thoughtful and judicious use of proxy variables (thus keeping in mind the problem with $x_2$).  We further suggest that these behaviors can be understood as embodying generative criteria.  That is, good scientists recognize that, insofar as the model is intended to represent the data-generating process, effects shouldn't be used as predictors and proxy variables should be used thoughtfully and judiciously.  In what follows, for the sake of simplicity of presentation, we will assume that reasonable methods have been used to avoid these problems, and so it's reasonable to talk about, say, the contributions that $x_1$ makes to the data-generating process. 

Note that, on our analysis, bias is typically distinct from inaccuracy or predictive error.  Bias concerns the correct estimation of parameters that describe the whole population; predictive accuracy concerns the correct estimation of individual-level response or outcomes.[^bias-ml]  In practice, predictive accuracy can often be evaluated simply by using the model to generate predictions for a new sample or testing set.  But evaluating the bias of a modeling process requires comparing tendencies of its parameter estimates to known true values.  This requires either formal statistical analysis or the construction of simulated populations with known parameter values.  Formal statistical analysis is often well beyond the statistical skills of practicing scientists; and while the kind of programming skill needed to construct simulations is becoming more common among practicing scientists, it is still relatively rare.  In either case, there is a problem of extrapolation:  it will not necessarily be obvious that the real-world data generation process of interest — say, a particular health survey — is adequately represented by the assumptions of the formal analysis or simulation study.  

[^bias-ml]: In machine learning contexts, "bias" is often used to refer to the square difference between a modeling process's expected estimated response and the true response at a given input $x$, $(E[\hat y] - y)^2$.  This use of "bias" is, in our sense, both a bias and an accuracy calculation; the parameter here is the true response $y$, and a modeling process is biased insofar as the estimate $\hat y$ does not tend to converge to it. 


## Structural Uncertainty ##

As the title suggests, the overall aim of this paper is to apply another pair of concepts from the model-based-science literature to statistical models; namely, Wendy Parker's distinction between \technical{structural uncertainty} and \technical{parameter uncertainty} [@Parker:Predicting, 265].  Parameter uncertainty refers to "uncertainty about the values that should be assigned to parameters within a set of modeling equations" [@Parker:Predicting, 265].  Without too much simplification, parameter uncertainty is the challenge that statistical methods have been developed to address:  how do we reliably estimate parameters, and how do we quantify our uncertainty about those estimates, given a certain model of the data-generating process? 

Structural uncertainty, by contrast, refers to "uncertainty about the form that modeling equations should take (e.g. should this quantity be represented as a function of just variable x, or of both variable x and variable y)" [@Parker:Prediction, 265, Parker's brackets].  Beyond variable inclusion, structural uncertainty also includes the following issues:  
\begin{description}
\item[variable representation] Given that a variable is included in the model, it can be represented in different ways; variables can be represented as continuous or discrete, can be left in "natural units" or squared or cubed, or given a more flexible representation, such as a spline basis (discussed below). 
\item[variable interaction] Given two or more variables, they can be represented as independent components of the data-generating process, or as interacting. *[Tabery?]
\item[model specification] Different overall functional forms can be used to represent the relationship between the variables.  The variables might be represented as additively contributing to the data-generating process, or multiplicatively.  The contributions of variables might be constant across the entire domain, or might vary.  
\end{description}
This kind of uncertainty can be understood with reference to both predictive and generative criteria.  Predictively, we can ask how accuracy and error depend on the model's structure; generatively, we can ask what model structure is most faithful to the processes at work within the target system; namely, for statistical models, what model structure is most faithful to the data-generating process.  

Two additional kinds of uncertainty will be relevant below.  First, \technical{evaluative uncertainty} is uncertainty about whether, in a particular case, a model (or modeling process) has certain desirable features, such as unbiasedness.  For example, with simple random sampling, the arithmetic mean (add the values and divide by the number of observations) is an unbiased estimator of the population mean.  However, if her dataset were collected using a complex survey design, the statistical practitioner might be uncertain whether the arithmetic mean is still unbiased.  Evaluative uncertainty will be especially important when the statistical practitioner has limited formal training in statistics.  Second, \technical{computational uncertainty} is uncertainty about whether a particular analytical procedure — say, a particular block of computer code — actually implements the calculation that it is designed to implement.  Computational uncertainty might occur because certain tools in the procedure as used as black boxes — say, the code uses functions from a software package that the statistical practitioner herself does not understand.  However, computational uncertainty might also occur because of limited skill of the designer — they are unable to design and run appropriate automated tests, for example.  

Above, we noted that bias is often much harder to evaluate than predictive accuracy, and specifically many practicing scientists do not have the skills needed to evaluate bias, either formally or using simulations.  This means that, in many cases of actual statistical practice, there will be both evaluative and computational uncertainty about the generative fidelity of the statistical models.  

Structural uncertainty in statistical models can be recast as the model selection problem:  given several different models, which one should be selected and used for inductive and abductive inference?  Structural uncertainty might therefore be addressed by appealing to some combination of (1) philosophical discussions of curve fitting, (2) statistical methods for model selection, or (3) disciplinary conventions.  

Below, we respond to (1) and (2) by showing that different statistical methods for model selection lead to discordant results in our case study.  Here we respond to (3), appeals to disciplinary conventions.  According to these appeals, scientific disciplines or research communities generally have formal or informal standards for statistical modeling, at least for normal problems (in the Kuhnian sense) in the field:  when doing this kind of study, include these control variables in this kind of model, use this method to test for inclusion/exclusion of variables, and so on. Given model uncertainty, these standards are \technical{conventions}.  A standard example of a convention is driving on the left or right side of the road.  There don't seem to be deeply compelling reasons for preferring one side to the other; but avoiding accidents requires that drivers in a given region all drive on the same side; so once one standard or the other has been adopted, all drivers are required to comply with it (indeed, can be punished for failing to comply with the standard).  As Wilholt puts it, "The standards adopted are arbitrary in the sense that there could have been a different solution to the same coordination problem, but once a specific solution is socially adopted, it is in a certain sense binding" [@Wilholt2009, 98].  Specifically, with statistical modeling, there may be multiple equally good or incommensurable mathematically possible modeling approaches for the same data set and basic research question; but once the scientific community has settled on one approach as the convention in the field, individual scientists or research teams should follow that convention.  

Appeals to disciplinary conventions might be motivated in two ways.  On the one hand, what we might call \technical{the Burkean argument} hypothesizes that disciplinary conventions become established over time as the community of researchers gradually and informally identify "what works" for their specific epistemic and pragmatic goals.  Critically for the Burkean argument, this gradual and informal process means that the community does not systematically record what alternatives were tried, or what compelling reasons there were for adopting one convention rather than another.  Consequently, members of the community of researchers today are not in a good position to evaluate the conventions of their discipline.  We have the conventions that we do because our forerunners found that they work; but we are not able to know \emph{why} they work.  Further, alternative conventions are expected to be inferior to the established ones, though their inferiority might not be obvious until some time after they are adopted.  So we should generally defer to the "wisdom of the ages," and use the established conventions of our community of researchers.  

Specifically, in the context of statistical modeling, disciplines generally have established modeling practices for paradigmatic problems:  include these control variables in this kind of model, use this method to decide whether to include/exclude these variables, and so on.  The Burkean argument hypotheses that these modeling conventions developed because they work, but that we are not in a good position to understand why they work, and so we should use the established conventions without question.  

There are a number of problems with the Burkean argument.  First, as the reference to Edmund Burke suggests, it is an epistemologically or methodologically conservative position.  This conservatism itself may be unattractive to many philosophers and scientists interested in methodological innovation and progress.  Second, the argument assumes a selectionist model of the history of science — that various alternatives are tried, and "survival of the fittest" ensures that the conventions that survive are the best ones.  But of course the history of science shows us a much messier, path-dependent process.  Even if we grant that the history of science has — over sufficiently long time frames — generally moved towards better methods, this does not mean that it has identified the best ones.  Third, the argument assumes that we are ignorant of the history by which those conventions were established — that we cannot examine what alternatives or tried, and why one alternative became established rather than another.  But of course the field of history of science does exactly this *[HicksStapleford].  Specifically, there are a number of excellent histories of the development of statistical conventions in the twentieth century *[Gigerenzer et al], which reveal the influence of epistemically questionable factors such as strong personalities and Cold War funding regimes.  And fourth, the argument fails to recognize that the goals of research communities change over time.  Even if we grant that established conventions worked well for achieving the goals of the research community at one time, this does not mean that the same methods will work well for achieving different goals of the same community later.  T-tests with a .05 threshold for statistical significance might be appropriate for analyzing relationships between single genes and single phenotypic traits, but lead to disaster in the context of genome-wide association studies involving thousands of genes [https://bmcgenomics.biomedcentral.com/articles/10.1186/1471-2164-11-724].  

Torsten Wilholt and Stephen John *[independently?] offer a different appeal to disciplinary conventions *[cites].  They argue that divisions of epistemic labor — between scientists and "the public," between scientists of different disciplines, and even between scientists of the same discipline working together — create epistemic dependencies and in turn a requirement for epistemic trustworthiness.  That is, we must be able to trust the findings that you communicate to me, even if we do not understand the methods by which you reached those findings.  Disciplinary conventions support this trustworthiness by justifying expectations and support shared interpretations.  In terms of statistical modeling, we may not be able to understand why researchers in your field use certain conventions when you design statistical models; but (assuming the conventions are designed and followed correctly) we do understand how to interpret your finding that the coefficient of a certain regression model term has a 95\% confidence interval of .67-.93, and I am justified in using this finding in my own work.  

We have three responses to this appeal to disciplinary conventions.  First,  disciplinary conventions are only justified insofar as they are indeed designed and followed correctly.  A number of statisticians and methodologists have criticized what Gigerenzer calls the "mindless" use of p-values and textbook statistical tests *[cite].  Insofar as these techniques are used without considering whether their assumptions are approximately satisfied, they can be much less epistemically trustworthy than their formal statistical properties indicate.  More fundamentally, the conventions themselves can be misdesigned.  For example, in psychology, biomedical research, and other fields, "statistical significance" (strictly, a p-value calculation of less than .05) is required for publication; studies with only "null results" effectively cannot be published *[cite].  This conventional publication standard produces substantial bias (in the statistical sense), which in part has contributed to the replication crisis in these fields *[Romero 2016].  

Second, these disciplinary standards may be laden with objectionable values.  For example, racist, sexist, classist, and ableist assumptions were common in medical, behavioral, and social sciences from the development of these fields as disciplines in the nineteenth century until recently (if not still today) *[cites].  Disciplinary conventions with morally objectionable assumptions are also untrustworthy.  For instance, in the context of environmental public health, reducing false negatives (incorrectly concluding that a chemical is safe when in fact it's hazardous) is more important than reducing false positives (incorrectly concluding that a chemical is hazards) [@Hicks2018].  Yet disciplinary conventions and regulatory guidelines focus on thresholds for statistical significance and other ways of avoiding false positives, rather than reducing false negatives. 

Third, the thought that disciplinary conventions can support shared interpretations is difficult to reconcile with the fact of disciplinary pluralism.  Different disciplines have different conventions; these differences can lead to misinterpretations or even incompatible bodies of evidence *[epistemic depth].  Patterns of misinterpretations and "epistemic depth" *[cite again] will tend to undermine trust, not bolster it.  


# Model Construction #

Our primary argument for structural uncertainty in statistical modeling is empirical:  we give an example of this uncertainty in a realistic case study.  Specifically, in this section we construct regression models of the health impacts of obesity.  By identifying distinct sources of model uncertainty, we construct a total of *[16]* different models, which can be evaluated (for model selection) using *[4]* different fidelity statistics.  We use real data, from a survey that is widely used by epidemiologists and public health officials; and — other than constructing multiple models — we use standard and recommended statistical methods.  In the next section, we show how these models give disparate indications of the health impacts of obesity.  

## Data ##

We used data from the National Health and Nutrition Examination Survey (NHANES), an ongoing health survey conducted by the US National Institutes of Health *[cite]*.  NHANES collects clinical, behavioral, demographic, and socioeconomic data on respondents across the United States; is anonymously linked to mortality data curated by the Centers for Disease Control and Prevention (CDC); uses a complex survey design to permit better statistical inferences about both the US population as a whole and smaller subpopulations (e.g., racial and ethnic minorities); and is freely available to the public.  For these reasons, NHANES is widely used in epidemiology and public health.  For example, the original "obesity paradox" paper by Flegal et al *[cite] is based on NHANES data.  

NHANES was initially conducted as a series of independent surveys; NHANES III, the third such survey, was conducted from 1988-1994.  Beginning in 1999, NHANES moved to a continuous cycle format, with new data released every two years. *[https://wwwn.cdc.gov/nchs/nhanes/Default.aspx]  Here we use NHANES III and cycles 1-3 of continuous NHANES, covering 1999-2004.  Followup mortality data were collected in 2011 *[https://www.cdc.gov/nchs/data-linkage/mortality-public.htm].  Both NHANES III and continuous NHANES use a complex survey design that oversamples "black and Mexican-American" people, to allow reliable inferences about these minority groups  *[https://wwwn.cdc.gov/nchs/data/series/sr02_113.pdf 1; @Lumley:Complex].  

Following studies in the "obesity paradox" literature *[cites], we focus our analysis on the association between BMI and all-cause mortality, incorporating several social and demographic variables into our models as controls: dichotomous sex, race/ethnicity *[note probs w/ cite to Sean], education level.  Individuals were included if they were between 50 and 85 years old when they participated in NHANES, and "aged out" of the study at 85.  This means that we consider a dead individual to be "alive" so long as they died after their 85th birthday.  For the Cox proportional hazards model (see below), age at followup (either age at death or age in 2011) was used as survival time; for the other models, age at followup was included in the covariates as another control.  

This age filtering introduces a potential for bias in several of our models.  To understand this bias, consider an overweight individual who was 75 and alive during followup in 2011, but who died in 2012.  Incorrectly treating this individual as alive will tend to make the estimates for the overweight group slightly more optimistic than the true value.  This phenomenon is called \technical{censoring}; while survival models (such as the Cox proportional hazards model) are designed to take it into account, the other models we use are not.  However, as we discuss below, survival models are also designed around a different kind of endpoint or response — relative survival time rather than whether an individual is alive or dead — which may make them inappropriate for certain studies.  Censoring can be avoided using a different study design.  For example, we might include only individuals who were 50 years old when they participated in NHANES and would have been at least 85 in 2011.  While *[our analysis file] can construct and analyze this kind of \technical{cohort sample}, using NHANES data it produces small samples and highly uncertain estimates.  This is NHANES is only a few decades old — not many participants have had a chance to age from 50 to 85.  Censoring means that our generalized linear models also have *[x] uncertainty — we are uncertain whether or to what extent the estimates produced by these models are biased.  

We apply additional filtering to the NHANES sample, again following the lead of studies in the "obesity paradox" literature.  Specifically, we include only individuals who (1) report never smoking cigarettes, (2) have BMI less than 75 (for a person 5'6"/1.67m tall, this corresponds to a cutoff of about 460 lbs/210 kg), (3) survived at least one month after participating in NHANES, and (5) have data for BMI, education level, and followup mortality status.  All together, the dataset we use includes 5,677 individuals.  Summary statistics of this dataset are included in the *[supplemental file].  

Clearly, both the choice of variables and the use of filtering criteria introduce what we might call \technical{data collection uncertainty}.  Note that this uncertainty is not due to limited precision in measurement — that is, \technical{measurement uncertainty} — or questions about the extent to which the sample accurately represents a target population — \technical{inductive uncertainty}.  These three kinds of uncertainty are also significant and worth further analysis; however, since our focus is on model-related uncertainty we do not discuss them further here.  

## Models ##

We focus here on three sources of model uncertainty, which we call \technical{covariate specification}, \technical{model specification}, and \technical{predictive fidelity}.  To examine the effects of these sources of uncertainty, we systematically consider several variations under each source.  *[These variations are summarized in table 1.] 

| **Covariate Specification**  | **Model Specification** | **Predictive Fidelity** |
|:-----------------------------|:------------------------|:--------------------|
| Binned or discrete BMI       | Linear                  | Accuracy            |
| Continuous BMI               | Logistic                | Precision           |
| Square BMI                   | Poisson                 | Recall              |
| 4-knot spline                | Cox PH                  | $F_1$               |
||                                                       | AIC                 |
||                                                       | AUROC               |

Table 1: Variations used for each source of model uncertainty.  The columns are read independently, giving a total of 16 models (for the first two columns) that are evaluated using 6 different predictive fidelity statistics.  

Before discussing these sources of uncertainty, note that we do not consider interactions between covariates, which are another source of structural uncertainty for statistical models.  In the context of human genetics, James Tabery argues that gene-environment interactions are highly variable:  they are substantial in some contexts, but not others *[cite].  From a statistical model-building perspective, including interactions will generally increase predictive fidelity, but may or may not be generatively faithful.  Since our case study uses one key predictor (BMI), and the other covariates are regarded as control variables, we do not consider any interactions here.  However, interactions would be highly appropriate if and insofar as we suspect, for example, that the effect of BMI on all-cause mortality depends on sex, race, or socio-economic status.  *[In our examination of the "obesity paradox" literature, we found that statistical models generally did not include interactions.]  

### Covariate Specification ###

By \technical{covariate specification}, we refer to the way the independent variables in a statistical model are specified.  Consider a seemingly-simple univariate linear regression model:  

$$ Y = \beta X + \beta_0 $$

Even given this model specification, there may be several different ways of specifying the independent variable or covariate $X$.  These different specifications of $X$ may lead to different conclusions about the relationship between $X$ and $Y$.  

In the obesity literature, it is common to use a binned or discretized version of BMI.  Since BMI is the quotient of two continuous-valued variables, it is itself continuous-valued.  But this continuous value is conventionally divided into several bins, shown in table 2.  Thus, our first two variations for covariate specification are (1) continuous BMI and (2) discrete BMI.  

| BMI bin name | BMI range    |
|:-------------|:-------------|
| underweight  | $<$ 18.5     |
| normal weight| 18.5 - 25    |
| overweight   | 25 - 30      |
| obese I      | 30 - 35      |
| obese II     | $>$ 35       |

Table 2: Conventional bins for BMI values 

The use of discrete BMI may strike some readers as an implausible oversimplification.  While individuals with BMI 25 (on the low end of the "overweight" bin) may be similar to individuals with BMI 30 (on the high end of the "overweight" bin), it is plausible that they are more similar to individuals with BMI 24 (in the "normal weight" bin).  These cutoffs coincide very neatly with our base-10 number system, which may make us suspect that they do not correspond to any real biological differences.  That is, we may suspect that discrete BMI has low generative fidelity.  

On the other hand, the use of discrete BMI can be justified by a suspicion that the relationship between BMI and mortality is not linear.  In the univariate linear model above, increasing the value of $X$ by one unit corresponds to an increase in $Y$ of $\beta$ units.  This is the case whether the starting value of $X$ is large or small.  In terms of BMI, increasing BMI from 19 to 23 would have exactly the same effect as increasing BMI from 33 to 37.  But it is plausible that these two changes in BMI might have different effects.  With discrete BMI, we can estimate effects relative to a baseline level (in this case, "normal weight") without assuming a linear relationship.  This approach will not let us make claims about changes in continuous BMI, e.g., the effect of a decrease from 37 to 33.  But it will let us make claims about whether, say, "obese II" individuals are at greater or lower risk than "normal weight" individuals.  This can be seen as a tradeoff between generative and predictive fidelity, or between realism and flexibility *[cite Weisberg]:  continuous BMI is more realistic or generatively faithful; while discrete BMI might be more flexible in ways that facilitate better predictive fidelity.  

Non-linear covariate specifications give us another way to balance generative and predictive fidelity.  One common approach is to add exponentiated terms — squares, cubes, square roots, etc. — of covariates of interest.  Our third covariate specification includes both continuous BMI and its square, $X + X^2$.  

Another family of non-linear covariate specifications uses \technical{splines}, or piecewise polynomials. *[cite]  Suppose a given covariate $X$ takes values in the interval $(0,1)$.  This interval can be divided into $k+1$ sub-intervals by setting $k$ breakpoints or *knots* within this interval:  $t_0 = 0 < t_1 < t_2 < \cdots < t_k < 1 = t_{k+1}$.  Polynomial functions $p_j(x)$ (typically with a fixed degree, say $d=3$ for \technical{cubic splines}) are constructed for each subinterval $[t_j, t_{j+1}]$ such that $p_j(t_{j+1}) = p_{j+1}(t_{j+1})$.  That is, at each knot point $t_j$, the two polynomials agree, ensuring that the combined function over the entire interval $(0,1)$ is continuous.  

Splines allow us to treat BMI as a continuous variable while also allowing the relationship between all-cause mortality and BMI to vary non-linearly.  Our fourth covariate specification uses a 4-knot spline for BMI with cubic splines.  To set the knots, we first estimate the distribution of BMI values for the population (not just the survey sample), then set the knots at the 20th, 40th, 60th, and 80th percentiles for this distribution.  A 4-knot specification roughly corresponds to the structure of conventional discrete BMI:  discrete BMI has 5 bins, with 4 breakpoints.  However, the location of the knots do not necessarily correspond to these breakpoints.  (Indeed, the knots are at about BMI 24, 26, 29, and 33.)  

## Model Specification ##

By \technical{model specification}, we refer to the mathematical structure of the statistical model other than the covariate specification.  Perhaps the most widely-used statistical model specification is the standard linear regression, where the response $Y$ is represented as a linear combination of the covariates:  

$$ Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \cdots.$$

However, this model specification does not fit easily with our response variable $Y$, all-cause mortality — that is, there are serious limits to its fidelity to the data-generating process.  $Y$ can take exactly two values:  alive or dead.  These two values can be represented numerically as 0 or 1.  But predicted values $\hat y$ — estimates of the individual state $y$ produced using the model — are not limited to these two values.  When the predicted values $\hat y$ are between 0 and 1, they might be interpreted as probabilities — for example, a predicted value $\hat y = .25$ can be interpreted as a 25% chance of being dead.  But, even then, the standard linear regression does not place bounds on the range of $\hat y$.  So for sufficiently large (positive or negative) values of the covariates, $\hat y$ will be less than 0 or greater than 1.  Obviously these values cannot be interpreted as probabilities.  

*Generalized linear models* were developed to build models that better fit response variables like our $Y$.  The observed values $y$ are modeled as samples drawn from some probability distribution $Y$.  For example, our observations $y$ — alive or dead — might be modeled as a series of Bernoulli trials, that is, single flips of a biased coin; the bias $\theta$ is the probability that the coin lands heads.  (Note that this use of "bias" shouldn't be confused with biased or unbiased estimates.)  That is, we might model the series of observed values $y$ as samples drawn from a Bernoulli distribution $Y$ with unknown bias $\theta$: $\y \sim Y(\theta)$.  In terms of fidelity to the data-generating process, this fits all-cause mortality much better than the continuous, unbounded response variable in the standard linear regression.  (Since a series of Bernoulli trials follows the binomial distribution, this way of modeling the response is often called \technical{binomial regression}.)  

To connect the response distribution $Y$ to the covariates $X$, we use an invertible \technical{link function} $g$:  

$$ E[Y] = g^{-1}(\beta_0 + \beta_1 X_1 + \beta_2 X_2 + \cdots)$$

where $E[Y]$ is the expected value of $Y$.  For the Bernoulli distribution with bias $\theta$, $E[Y] = \theta$; different covariate values correspond to different values of $\theta$ via the link function $g$.  That is, the covariates determine a key property of the distribution that generates the observations — the unobserved bias in the "coin flip."  

For binomial regression, a standard link function is the logit function

$$ g(p) = \log\frac{p}{1-p}. $$

The inverse $g^{-1}(x) = \frac{e^x}{1+e^x}$ is called the logistic function; so a binomial regression with a logit link is often called simply \technical{logistic regression}.  Note that the logit takes a probability $p$ to the logarithm of its odds, or log-odds.  So logistic regression connects the covariates to the log-odds of the bias $\theta$ of the "coin flips" generating the observed response values $y$.  Further, 

\begin{align} 
    odds(\theta) &= e^{\beta_0 + \beta_1 X_1 + \beta_2 X_2 + \cdots\\
        &= e^{\beta_0} \cdot e^{\beta_1 X_1} \cdot e^{\beta_2 X_2} \cdots\\
        &= e^{\beta_0} \cdot (e^{\beta_1})^{X_1} \cdot (e^{\beta_2})^{X_2} \cdots.
\end{align}

In the standard linear regression, for a given covariate, the ceteris paribus effect of that covariate (that is, keeping the values of all other covariates fixed) is linear.  And these effects are added together to predict the response.  By contrast, in logistic regression, the ceteris paribus effect of a covariate is exponential, and these effects are multiplied together to predict the response.  This means that the coefficients in the two model specifications — the $\beta$s — are incommensurable.  We cannot compare $\beta_1$ in a standard linear regression to $\beta_1$ in a logistic regression and ask whether their values are "the same" or "different."  

The first two model specifications that we consider here are a standard linear regression and logistic regression.  Logistic regression clearly is much more faithful to the data-generating process; but in actual scientific practice standard linear regressions are widely used even when logistic regressions would be more faithful.  On the other hand, standard linear regressions are more familiar to researchers and, perhaps for this reason, can be easier for researchers to interpret.  Further, and most importantly for our argument here, using generalized linear regression models introduces further sources of structural uncertainty, as different link functions can be used for a given way of modeling the response.  For example, besides the logit link, binomial regression can be used with the probit link — the probit is the inverse of the cumulative distribution function for the standard Gaussian ("normal") distribution — or the complementary log-log link:  
$$ g(p) = \log(-\log(1-p)) .$$

Our response variable — alive or dead — can also be represented as a discrete count variable — the "number of deaths" that occur for the given individual between their participation in NHANES and the CDC mortality followup.  The Poisson distribution is commonly used to represent discrete count variables, and so can be used to construct an alternative generalized linear model.  The Poisson distribution is described by a single positive real-valued parameter $\lambda$, gives the expected value, i.e., the expected number of events.  The natural logarithm is the standard link function for a \technical{Poisson regression}.  This gives us our third model specification.  

It might be objected that Poisson regression is clearly not faithful to the data-generating process, and so should not be used when we assemble our set of models.  However, biostatistician Guangyong Zhou has argued that a modified version of Poisson regression can be used with binary response variables.  Briefly, Zhou argues that, for a given covariate $X$ and its coefficient $\beta$ in a Poisson regression, the relative risk (difference in the response) associated with an increase in $X$ is simply $e^\beta$.    *[https://academic.oup.com/aje/article/159/7/702/71883/A-Modified-Poisson-Regression-Approach-to; http://journals.sagepub.com/doi/abs/10.1177/0962280211427759]  Zhou's regression is modified by using a different calculation of variance, which changes the standard errors of the coefficients but not their point-estimates.  That is, off-the-shelf Poisson regression incorrectly estimates the parameter uncertainty for Zhou's modified Poisson regression; but gives the same point-estimates.  

Our fourth model specification takes a conceptually different approach to the response variable.  Rather than treating it as a discrete alive or dead outcome, \technical{survival analysis} treats the response variable as \technical{survival time}, or the amount of time it takes for an event to occur to event (in our case, age at death).  More precisely, survival analysis is interested in modeling a hazard function $\lambda(t)$, which gives the probability of an event occurring at time $t$.  In the context of a regression, we model the hazard function in terms of covariates $X_1, X_2, \ldots$ and a baseline hazard function $\lambda_0$:  

$$ \lambda(t) = \lambda_0(t) e^{\beta_1 X_1 + \beta_2 X_2 + \cdots}.$$

Different approaches to hazard analysis use different methods to estimate the baseline hazard function $\lambda_0$; these introduce further structural uncertainty.  To avoid this structural uncertainty, the statistician David Cox developed a method, now called \technical{Cox proportional hazards regression} or sometimes simply \technical{Cox regression}, that instead focuses on estimating the relative hazard or risk:  

$$\frac{\lambda(t)}{\lambda_0(t)} = e^{\beta_1 X_1 + \beta_2 X_2 + \cdots}.$$

Here the response variable is not the hazard at time $t$, but instead the hazard at time $t$ relative to the baseline hazard (i.e., when all of the covariates are equal to 0).  Thus, while the right-hand side of the Cox model is similar to the right-hand sides of the logistic and Poisson regression, coefficients between these models cannot be compared:  for a given covariate $X_1$, it does not make sense to ask whether its coefficient $\beta_1$ in one model is greater, or less, than its coefficient in a different model.  

Cox proportional hazards is widely used in epidemiology.  Indeed, *[cites to obesity paradox literature]

All together, with four ways of specifying the key covariate, BMI, and four ways of specifying the other features of the statistical model, we have 16 distinct regression models of the relationship between BMI and all-cause mortality.  

## Model Selection ##

We compare these 16 models using six different *model selection* or goodness-of-fit statistics:  accuracy, AIC, AUROC, $F_1$, precision, and recall.  Since we have already discussed these fidelity criteria for statistical models above *[xref], we do not discuss them in detail here.  However, two aspects of model selection for our particular dataset and models are worth noting.  

Accuracy, precision, recall, $F_1$, and AUROC are calculated on a testing subset of the data, while AIC is calculated on the training subset used to construct the models.  Because NHANES uses a complex survey design, we cannot use simple random sampling across all of the observations to construct training and testing subsets.  Instead, we construct these sets using weighted sampling within each pseud-PSU *[cites to NHANES sampling methodology, Lumley]*; use subsetting methods included in the \texttt{survey} package to adjust sampling weights for each subset; and then calculate Horvitz-Thompson estimates for accuracy, precision, recall, and $F_1$ at the population level *[Lumley]*.  Approximately 70% of the individuals in each pseudo-PSU are in the training set.  Because we are not fully confident that this is the right way to construct training and testing subsets on complex survey data, and wrote the Horvitz-Thompson estimator calculations ourselves, there is both evaluative and computational uncertainty about these estimates.  The package we used to calculate AUROC did not allow us to use sampling weights, so AUROC estimates have evaluative uncertainty.  

Second, for AIC, we use methods included in the \texttt{survey} package.  However, as noted above, AIC comparisons are only meaningful when the response variable is specified in the same way.  In the case of our models, this means AIC cannot be used to compare the Cox model with generalized linear regressions; and there is evaluative uncertainty about whether AIC cannot be used to compare generalized linear regressions with different model specifications.  AIC can still be used to compare models with the same model specification but different covariate specifications.  


# Model Analysis #

In this section, we analyze our 16 models in terms of generative fidelity, by calculating our different predictive fidelity or model selection statistics, and by generating relative risk predictions.  In each case, we argue that there is significant ambiguity or incommensurability.  

## Generative Fidelity ##

In terms of generative fidelity, it is highly plausible that the Cox proportional hazard models are the most faithful to the data-generating process.  Among the generalized regression models, linear regression represents binary mortality status as an unbounded, continuous variable; and Poisson regression represents mortality status as a count variable, with no upper bound.  While the logistic regression models are more faithful to the binary mortality status response variable, our dataset also has censoring.  As discussed above, censoring can introduce bias into generalized linear models, but survival models such as the Cox model are designed to account for censoring.  

Because none our models include interactions, the covariates are represented as independent contributors to all-cause mortality.  It is at least reasonable to think that this assumption is suspect, and thus that all of our models share this limitations on their generative fidelity.  On the other hand, from a model selection perspective, this point does not help us choose between the available models.  

Generative fidelity is much less clear between the different covariate specifications.  Simply put, without an independent, mechanistic understanding of how BMI might relate to mortality in general, it is difficult to say which covariate specification best represents that relationship statistically.  It is at least highly plausible that discrete BMI is the least generatively faithful — both the number of bins and the thresholds between them appear to be arbitrary.  And because it requires a linear or uniform effect, it is also plausible that continuous BMI has low generative fidelity.  This suggests that, among the four available options, the square and spline specifications may have the highest generative fidelity.  But square specifications also seem arbitrary — why think that mortality varies with the square of BMI, but not the cube or the square root or other power?  And splines are at once both highly complex and highly simple.  On the one hand, they are highly complex, in that specifying splines requires specifying the number and location of knot points and the polynomial degree.  It is plausible that it would be extremely difficult to get good empirical estimates of the correct values for these parameters.  On the other hand, splines are highly simple, in that they are piecewise polynomials.  While polynomials are good at approximating other functions[^dense], and extremely convenient for human- and machine-accessible representations, they have a number of unusual properties as functions, and in this sense are "simple."  Why should we think that the true relationship between BMI and all-cause mortality can be expressed using this kind of "simple" function?  This question points us towards the deep debate over the role of simplicity in scientific reasoning *[cites].  

[^dense]: The Weierstrass approximation theorem states that polynomials are dense in the space of continuous functions on a closed real interval $[a,b]$.  


## Predictive Fidelity ##

In line with our discussion in *[xref], all of the models produce continuous-valued predictions $\hat y$, and calculating accuracy, precision, recall, and $F_1$ requires setting a discrimination threshold to map these predictions into the set $\{alive, dead\}$.  For each model, we set the threshold so that the total number of predicted deaths is equal to the total number of observed deaths in the sample:  $\sum_i \hat y_i = \sum_i y_i$.  In what follows we use this threshold for accuracy, precision, recall, and $F_1$.  AIC and AUROC do not require choosing this threshold — but, as noted above, AIC cannot be used to compare the Cox models with the generalized linear models, and we are not certainty whether it can be used to compare across model specifications at all.  

Figure 1 shows the values of each model selection statistic for all 16 models.  In each panel of this plot except AIC, points located higher on the y-axis correspond to better scores.  The x-axis in this plot corresponds to dataset; since only the censored dataset was used (and the cohort-based dataset was not used), this axis can be ignored.  *[In the online supplement, Table S1 gives numerical values from this plot in a sortable list.]  

![Figure 1. Predictive fidelity statistics for all 16 models.  In each panel except AIC, higher values indicate greater predictive fidelity.  The x-axis in each panel corresponds to the dataset; since only the censored dataset was used here, this axis can be ignored.](01_pred_fit.png)  

By every metric, the Cox models perform much worse than any of the generalized linear models.  For AUROC, the other three specifications perform roughly equally well.  For the thresholded accuracy statistics, both covariate specification and model specification seem to make a difference:  logistic and Poisson models with continuous BMI seem to do the best overall, but linear models with continuous BMI generally do worse than spline models.  This might be due to flexibility — splines allow non-linear relationships between BMI and mortality — but linear spline models seem to perform better than logistic and Poisson spline models.  

AIC should be interpreted with care.  For the generalized regression models, the response variable is whether or not the survey respondent died.  On the other hand, for the Cox models, the response is the amount of time the respondent survived until either dying, being lost to followup, or aging out of the study.  So it is presumably illegitimate to compare the AIC of the Cox model to the AIC of the other three model specifications.  AICs for those other specifications are probably be comparable — but we are not certain about this, so there is broad evaluative uncertainty about the use of AICs for model selection here.  

Setting side the Cox model, the distribution of AICs contrast sharply with those of the other statistics.  Variable seems to matter little or not at all; the plot gives the impression that the AIC values are the same for a given specification, though *[Table S1] indicates small differences.  Specifically, linear models appear to be uniformly better the other specifications according to AIC; and yet for many covariate specifications linear models often have worse accuracy on the testing set than Poisson and logistic models.  

All together, these statistics profoundly underdetermine model selection.  AIC favors linear models and is basically undecided between different covariate specifications; AUROC slightly favors binned BMI, slightly disfavors continuous BMI, and is undecided between different model specifications; and the other accuracy statistics consistently support logistic and Poisson models with continuous BMI, and generally disfavor binned BMI.  In addition, there is evaluative uncertainty about both AIC (because it is not clear whether it can be used to compare different generalized linear models) and AUROC (because there was no way for us to include sampling weights), and computational uncertainty about the other accuracy statistics.  About all we can say with confidence is that the Cox models have poor predictive accuracy.  

This underdetermination of model selection by different predictive accuracy statistics means that philosophical and statistical approaches to model selection exacerbate, rather than reduce, structural uncertainty.  Indeed, we can understand the variety of predictive accuracy statistics as structural uncertainty about the concept of "predictive accuracy."  Should accuracy be understood in terms of divergence — which generalizes OLS accuracy and is directly minimized by maximum likelihood methods — or in terms of the correct prediction rate — which requires setting a prediction threshold and choosing the relative weight of false and negative values?  *[more?]



## Relative Risk Predictions ##

Figures 2 and 3 each show relative risk predictions for all 16 models across a range of BMI values, with all other covariates held fixed.  These plots thus show the statistical effect of changes in BMI, according to the various models.  Risk is measured relative to a baseline BMI of 22; that is, the risk for BMI = 22 is stipulated to be 1.0; relative risk values less than 1 indicate deceased risk relative to BMI = 22, while values greater than 1 indicate increased risk.  

![Figure 2. Relative risk predictions for all 16 models.  All covariates other than BMI are held fixed.  Predictions are grouped by covariate specifications. Ribbons give 95% confidence intervals.  All predictions are the same as in figure 3.](02_rr_preds.png)

![Figure 3. Relative risk predictions for all 16 models.  All covariates other than BMI are held fixed.  Predictions are grouped by model specifications.  Ribbons give 95% confidence intervals.  All predictions are the same as in figure 2.](03_rr_preds.png)


All 16 sets of predictions are given in both figure 2 and figure 3; these predictions are grouped differently to aid interpretation.  In figure 2, predictions are grouped by covariate specification, allowing us to compare across model specifications; while predictions in figure 3 are grouped by model specification, allowing us to compare across covariate specifications.  

Figure 2 indicates that the Cox model consistently generates predictions of greater risk than the other specifications, especially for BMI values greater than 25.  Recall that the lower bound for overweight is 25.  Thus, the region for which the Cox model indicates greater risk (relative to the other model specifications) is the region for overweight and obesity.  The other three model specifications agree with each other quite closely across the whole range of plotted BMI values.  

Furthermore, the other three model specifications generally find evidence of an "obesity paradox."  For non-linear covariate specifications (i.e., binned, square, and spline BMI), the point estimates are below 1 from about 25 until 35 or greater.  There is no indication of the "obesity paradox" with continuous BMI; but since the "obesity paradox" is a non-linear trend, a continuous model cannot show it.  

However, the patterns discussed in the last two paragraphs could be due to bias.  Recall that only Cox models are designed to account for censored data.  At this point, the discussions of the last two sections lead to a major interpretive dilemma.  On the one hand, generative fidelity considerations strongly favor the Cox proportional hazards model.  On the other hand, predictive fidelity considerations strongly favor the generalized linear models, especially the logistic and Poisson regressions.  Insofar as these mortality predictions are interpreted as estimates of the true population risk ratios, then we should rely on the Cox models, and conclude that overweight individuals are at increased risk and that there is no "obesity paradox."  And insofar as these same mortality predictions are interpreted as instrumentalist predictions of risk ratios, then we should rely on the generalized linear models, and conclude that increased risk only appears at BMI 35 or greater and that there is a significant "obesity paradox."  *The two kinds of fidelity criteria, applied to the same set of relative risk predictions, indicate exactly opposite conclusions.* 

Finally, comparing figure 2 and figure 3, we note that covariate specification seems to matter much more than model specification in this particular case.  That is, given a covariate specification, the generalized linear models all tend to agree with each other (figure 2).  But, given a model specification, there is substantial disagreement across the different covariate specifications (figure 3).  We found this somewhat surprising.  Model specification is a frequent topic of discussion among statisticians and in statistical practice — which kind of model is appropriate for the kind of data you're working with?  For example, when one of us presented this paper to an audience of statisticians and statistical practitioners, roughly half of the discussion focused on whether linear and Poisson models should be included at all, because they are "obviously" inappropriate for binary response data.  There was no discussion at all of whether binning or splines might be more appropriate than continuous BMI.  And yet, in this case, linear and Poisson models produce output very similar to logistic regression, while the specification of BMI makes a dramatic difference to the model output.  The choice between logistic and linear regression matters much less than the choice between continuous and spline BMI.  



# Conclusion #


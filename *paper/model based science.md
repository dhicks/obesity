# Model-Based Science and Statistical Modeling #

## Statistical Models: Inferential Tools, not Models of Data ##

Many philosophers of science may assume a sharp distinction between ``models of phenomena'' — models as discussed in the model-based-science literature — and the models constructed in the practice of inferential statistics.  The latter statistical models may be regarded as ``models of data,'' which merely ``correct[], rectif[y], [and] regiment[]'' the raw data, then ``present the data in a 'neat' way, for instance by drawing a smooth curve through a set of points'' *[https://plato.stanford.edu/entries/models-science/#SemModRep].  The philosophically interesting questions about models of data, in this sense, might seem to be exhausted by discussions of trimming outliers and the role of simplicity in curve fitting *[https://plato.stanford.edu/entries/statistics/#StaMod].  

However, this account of statistical models as models of data — especially in an epistemically derogatory sense, as \emph{mere} models of data — seems to overlook two major features of their use in scientific practice.  First, statistical models are used for inductive and abductive inference:  to make predictions about future observations and to draw inferences about the values of (in some sense) unobservable variables.\footnote{Specifically, we think that statistical models are used abductively to make inferences about the ``true'' values of population parameters; for example, the population mean, as distinct from the mean observed in the available sample, or indeed any possible sample.  These parameters are unobservable in at least two senses.  First, any observations we make will be of a sample, finite, and influenced by measurement errors, limited precision, and other uncertainties.  We never directly observe the population itself, as it were.  Second, and more fundamentally, the population will often be counterfactual, such as an infinite population of US residents aged 18-84 in 2005.  While it's not clear to us that statistical populations are unobservable in the same sense that electrons are standardly taken to be unobservable [@vF:Scientific, 77], it does seem reasonable to characterize inferences about them as ``abductive.''}  Epistemically, this ampliative use of statistical models goes well beyond a neat presentation of data.  Second, classical statisticians (here including the few philosophers of statistics who have looked closely at actual statistical practice, but not including the machine learning community [@Breiman??, @Donoho??]) regard statistical models as \emph{models of the data-generation process} [@Suppes1962; @Mayo??; @Huebner??].  As Wasserman puts it, ``Statistical inference, or `learning' as it is called in computer science, is the process of using data to \emph{infer the distribution that generated the data}. A typical statistical inference question is: Given a sample $X_1, \ldots, X_n \sim F$, how do we infer $F$'' [@Wasserman:AllOf, 87, my emphasis; compare @Mayo??, 382, 384]?  (Here $\sim$ should be read ``is sampled from'' or equivalently ``is generated by.'')  Given that this generating distribution cannot be observed directly — all we have are the finite number of noisy observations in the sample — this second feature is closely connected to the abductive use of statistical models.\footnote{In the history of statistics, the view of statistical models as mere ``neat'' presentations of raw data seems to be associated with Karl Pearson, who viewed the aim of science as ``the discovery \ldots of a brief statement or formula, which in a few words resumes a wide range of facts'' [@Pearson:Grammar, ch. III §1].  However, this positivist interpretation of statistics was not shared with his contemporaries [@Pence:EarlyHistory, [8-9]] or intellectual (and biological) descendants [@Hodge??, especially 242; @Mayo??, ch. 11, esp. 380ff].}  

We believe that — while there are still important and interesting differences between statistical models and the ``models of phenomena'' examined in the model-based-science literature — this literature provides some conceptual tools that are useful for analyzing statistical models and their use in scientific practice.  

## Fidelity Criteria for Statistical Models ##

Specifically, given the two features identified above, in @Weisberg:Simulation's terminology, statistical models are \technical{target-directed}:  they are intended to "generate predictions and explanations about" their target systems [@Weisberg:Simulation, 74].  Successfully generating such predictions and explanations requires that the model satisfy some \technical{fidelity criteria}, which "describe how similar the model must be to the [target] in order to be considered an adequate representation [of the target]" [@Weisberg:Simulation, 41].  Weisberg distinguishes two types of such fidelity criteria, which in the context of statistical models we will call \technical{generative} (``representative'' in Weisberg's terminology) and \technical{predictive} (``dynamical'' in Weisberg's terminology).  

Predictive criteria evaluate a model in terms of the accuracy of its predictions about the target system.  For example, ordinary least squares regression [OLS] fits a line to a dataset that minimizes the error statistic $RSS = \sum_{i=1}^N (y_i - \hat y_i)^2$, that is, the sum of the square errors between the predicted values $\hat y_i = x_i^T\beta$ and the observed or true values $y_i$.\footnote{Throughout this paper, we use the statistical convention that hatted variables, $\hat y$, are predictions or estimates.}  In the typical case, minimizing this error statistic is equivalent to maximizing the log likelihood of the regression parameters $\beta$: $L(\beta) = \sum_{i=1}^N \log p(y_i | x_i^T\beta)$ [@Hastie:Elements 30-1].  In more general statistical modeling contexts — such as the use of logistic regression — this in turn is equivalent to minimizing the \technical{deviance}, defined simply as $-2L(\theta)$.  Deviance therefore generalizes the notion of error in OLS, and maximum likelihood methods can therefore be understood in terms of fitting a model by maximizing its accuracy.  *[AIC]

Other predictive criteria are also used in statistical modeling, corresponding to other ways of operationalizing accuracy or error.  In many cases the response variable (the ``$y$'') will be a binary positive or negative result, rather than a continuous value.  For example, in developing a diagnostic test for disease, the response may be that the patient or subject has the disease (``positive'') or does not (``negative'').  In this context, \technical{accuracy} is generally defined as the correct prediction rate ($p(\hat y = y)$); \technical{recall}, sensitivity, or the true positive rate is the probability that the model correctly predicts positive values ($p(\hat y = + | y = +)$); the false positive rate [FPR] is the probability that the model misses positive values ($p(\hat y = - | y = +)$); \technical{precision} or positive predictive value is the probability that the model's positive predictions are correct ($p(y = + | \hat y = +)$); and $F_1$ is the harmonic mean of precision and recall:  
\[ F_1 = 2 \frac{1}{\frac{1}{\textrm{precision}} + \frac{1}{\textrm{recall}}}. \]
Note that recall, precision, and therefore $F_1$ emphasize positive results, not negative ones.  For inductive risk reasons, these statistics will be less relevant in situations where false negatives are more important than false positives [@Hicks??].  

In the binary response case, the immediate predictions of models will generally be continuous values, say $\hat z \in (0,1)$.  Mapping these continuous values to the binary result, $\hat z \to \hat y \in \{-, +\}$, generally involves comparing $\hat z$ to a \technical{discrimination threshold} parameter $\alpha$:  if $\hat z > \alpha$ then $\hat y = +$; otherwise $\hat y = -$.  The value of $\alpha$ is generally free, meaning that there are little or no a priori constraints on how it should be set.  Since different values of $\alpha$ give different predictions, the accuracy and error statistics discussed in the last paragraph can take different values depending on $\alpha$.  The \technical{receiver operator characteristic} or ROC curve is defined as the curve $f(\alpha) = (\textrm{recall}_\alpha, \textrm{FPR}_\alpha)$.  The area under this curve, abbreviated AUROC or AUC, is frequently used as a threshold-independent accuracy statistic.  AUROC values range from 0 to 1; 0.5 corresponds to a ``naïve'' or ``random'' predictor, such as flipping a coin with bias $\alpha$.  AUROC values reportedly estimate the probability that an arbitrary positive will have a higher prediction score than an arbitrary negative *[http://pubs.rsna.org/doi/pdf/10.1148/radiology.143.1.7063747], that is, $p(\hat z_2 > \hat z_1 | y_2 = +, y_1 = -)$.  

Above, we noted that statisticians generally regard statistical models as models of the data-generating process.  For example, the sampling process is enormously important to many statisticians.  The basic suite of models taught in introductory statistics classes assume simple random sampling.  Under a range of conditions, these model can still produce unbiased point estimates given non-random or more complex sampling processes; but the standard errors (estimates of parameter uncertainty) will generally be incorrect, leading to incorrect abductive inferences about the populations from which the samples were drawn.  Complex survey designs and analysis techniques can be used to produce correct standard errors, and therefore better abductive inferences [@Lumley:Complex, 1-6].  In short, different data-generating processes (simple random sampling vs.~complex surveys) require different statistical models.  

Concern with representing the data-generating process corresponds to Weisberg's second kind of fidelity criteria, which we call generative criteria.  We are not aware of any examples, in frequentist statistics, of generative criteria that are operationalized as data-dependent statistics — there is nothing like RSS or the false negative rate for assessing the degree to which a model faithfully represents the data-generation process.  On the other hand, in hierarchical Bayesian methods, different data-generating processes can be represented as high-level parameters *[Kruschke? Gelman?].  For a simple (and somewhat unrealistic) example, suppose $\Omega \in \{0, 1\}$ is a high-level parameter representing whether the data were generated by simple random sampling ($\Omega = 0$) or a weighted sampling process ($\Omega = 1$).  If the sampling process is not known with certainty a priori and we can specify a model for $\Omega$ relative to the data $(y,x)$ — that is, if we can specify $p((y,x) | \Omega)$ — then we can estimate the posterior probability $p(\Omega | (y,x))$.  In both traditions, more typically generative criteria will take the form of mathematical assumptions about the statistical properties of the data-generating process.  For example, regression methods generally assume that predictor variables (the ``$x$``s) have independent causal relations with the response, which is operationalized as (defeasible) assumptions that the pairwise correlations of the predictors are all 0 and the coefficients on their interaction terms are all 0.  Note that this statistical representation of the data-generating process means that statistical models are generally not mechanistic; that is, they generally do not represent the data-generating process as a spatiotemporal arrangement of entities and activities [@LDC].\footnote{The language of a ``data-generating process'' lends itself to misleadingly simple causal inferences.  For example, in a linear model with $y = \beta_0 + \beta_1 x_1 + \beta_2 x_2$ with two predictors $x_1$ and $x_2$, if this model is read as a representation of the data-generating process, then it seems to follow that $x_1$ and $x_2$ are causes of $y$.  But of course $y$ might be a cause of $x_1$, and not vice versa; and $x_2$ might merely be correlated with some other variable $x'_2$ that is an actual cause of $y$ but is also much more difficult to measure than $x_2$.  We suggest that, in good cases of actual statistical practice, scientists draw on implicit or explicit background assumptions to select predictors that are causally upstream of the response (thus attempting to prevent the problem with $x_1$) and make thoughtful and judicious use of proxy variables (thus keeping in mind the problem with $x_2$).  We further suggest that these behaviors can be understood as embodying generative criteria.  That is, good scientists recognize that, insofar as the model is intended to represent the data-generating process, effects shouldn't be used as predictors and proxy variables should be used thoughtfully and judiciously.  In what follows, for the sake of simplicity of presentation, we will assume that reasonable methods have been used to avoid these problems, and so it's reasonable to talk about, say, the contributions that $x_1$ makes to the data-generating process.}  

## Structural Uncertainty ##

As the title suggests, the overall aim of this paper is to apply another pair of concepts from the model-based-science literature to statistical models; namely, Wendy Parker's distinction between \technical{structural uncertainty} and \technical{parameter uncertainty} [@Parker:Predicting, 265].  Parameter uncertainty refers to ``uncertainty about the values that should be assigned to parameters within a set of modeling equations'' [@Parker:Predicting, 265].  Without too much simplification, parameter uncertainty is the challenge that statistical methods have been developed to address:  how do we reliably estimate parameters, and how do we quantify our uncertainty about those estimates? 

Structural uncertainty, by contrast, refers to ``uncertainty about the form that modeling equations should take (e.g. should this quantity be represented as a function of just variable x, or of both variable x and variable y)'' [@Parker:Prediction, 265, Parker's brackets].  Beyond variable inclusion, structural uncertainty also includes the following issues:  
\begin{description}
\item[variable representation] Given that a variable is included in the model, it can be represented in different ways; variables can be represented as continuous or discrete, can be left in ``natural units'' or squared or cubed, or given a more flexible representation, such as a spline basis. *[explain splines below]
\item[variable interaction] Given two or more variables, they can be represented as independent components of the data-generating process, or as interacting. *[Tabery?]
\item[model specification] Different overall functional forms can be used to represent the relationship between the variables.  The variables might be represented as additively contributing to the data-generating process, or multiplicatively.  The contributions of variables might be constant across the entire domain, or might vary.  
\end{description}
This kind of uncertainty can be understood with reference to both predictive and generative criteria.  Predictively, we can ask how accuracy and error depend on the model's structure; generatively, we can ask what model structure is most faithful to the processes at work within the target system; namely, for statistical models, what model structure is most faithful to the data-generating process.  

Structural uncertainty in statistical models can be recast as the model selection problem:  given several different models, which one should be selected and used for inductive and abductive inference?  Structural uncertainty might therefore be addressed by appealing to some combination of (1) philosophical discussions of curve fitting, (2) statistical methods for model selection, or (3) disciplinary conventions.  

In the following sections, we respond to (1) and (2) by showing that different methods for model selection lead to discordant results in our case study.  Here we respond to (3), appeals to disciplinary conventions.  According to these appeals, scientific disciplines or research communities generally have formal or informal standards for statistical modeling, at least for normal problems (in the Kuhnian sense) in the field:  when doing this kind of study, include these control variables in this kind of model, use this method to test for inclusion/exclusion of variables, and so on. Given model uncertainty, these standards are \technical{conventions}.  A standard example of a convention is driving on the left or right side of the road.  There don't seem to be deeply compelling reasons for preferring one side to the other; but avoiding accidents requires that drivers in a given region all drive on the same side; so once one standard or the other has been adopted, all drivers are required to comply with it (indeed, can be punished for failing to comply with the standard).  As Wilholt puts it, ``The standards adopted are arbitrary in the sense that there could have been a different solution to the same coordination problem, but once a specific solution is socially adopted, it is in a certain sense binding'' [@Wilholt2009, 98].  Specifically, with statistical modeling, there may be multiple equally good or incommensurable mathematically possible modeling approaches for the same data set and basic research question; but once the scientific community has settled on one approach as the convention in the field, individual scientists or research teams should follow that convention.  

Appeals to disciplinary conventions might be motivated in two ways.  On the one hand, what we might call \technical{the Burkean argument} hypothesizes that disciplinary conventions become established over time as the community of researchers gradually and informally identify ``what works" for their specific epistemic and pragmatic goals.  Critically for the Burkean argument, this gradual and informal process means that the community does not systematically record what alternatives were tried, or what compelling reasons there were for adopting one convention rather than another.  Consequently, members of the community of researchers today are not in a good position to evaluate the conventions of their discipline.  We have the conventions that we do because our forerunners found that they work; but we are not able to know \emph{why} they work.  Further, alternative conventions are expected to be inferior to the established ones, though their inferiority might not be obvious until some time after they are adopted.  So we should generally defer to the ``wisdom of the ages,'' and use the established conventions of our community of researchers.  

Specifically, in the context of statistical modeling, disciplines generally have established modeling practices for paradigmatic problems:  include these control variables in this kind of model, use this method to test for inclusion/exclusion of variables, and so on.  The Burkean argument hypotheses that these modeling conventions developed because they work, but that we are not in a good position to understand why they work, and so we should use the established conventions without question.  

There are a number of problems with the Burkean argument.  First, as the reference to Edmund Burke suggests, it is an epistemologically or methodologically conservative position.  This conservatism itself may be unattractive to many philosophers and scientists interested in methodological innovation and progress.  Second, the argument assumes a selectionist model of the history of science — that various alternatives are tried, and ``survival of the fittest'' ensures that the conventions that survive are the best ones.  But of course the history of science shows us a much messier, path-dependent process.  Even if we grant that the history of science has — over sufficiently long time frames — generally moved towards better methods, this does not mean that it has identified the best ones.  Third, the argument assumes that we are ignorant of the history by which those conventions were established — that we cannot examine what alternatives or tried, and why one alternative became established rather than another.  But of course the field of history of science does exactly this *[HicksStapleford].  Specifically, there are a number of excellent histories of the development of statistical conventions in the twentieth century *[Gigerenzer et al], which reveal the influence of epistemically questionable factors such as strong personalities and Cold War funding regimes.  And fourth, the argument fails to recognize that the goals of research communities change over time.  Even if we grant that established conventions worked well for achieving the goals of the research community at one time, this does not mean that the same methods will work well for achieving different goals of the same community later.  T-tests with a .05 threshold for statistical significance might be appropriate for analyzing relationships between single genes and single phenotypic traits, but lead to disaster in the context of genome-wide association studies involving thousands of genes [https://bmcgenomics.biomedcentral.com/articles/10.1186/1471-2164-11-724].  

Torsten Wilholt and Stephen John *[independently?] offer a different appeal to disciplinary conventions *[cites].  They argue that divisions of epistemic labor — between scientists and ``the public,'' between scientists of different disciplines, and even between scientists of the same discipline working together — create epistemic dependencies and in turn a requirement for epistemic trustworthiness.  That is, I must be able to trust the findings that you communicate to me, even if I do not understand the methods by which you reached those findings.  Disciplinary conventions support this trustworthiness by justifying expectations and support shared interpretations.  In terms of statistical modeling, I may not be able to understand why researchers in your field use certain conventions when you design statistical models; but (assuming the conventions are designed and followed correctly) I do understand how to interpret your finding that the coefficient of a certain regression model term has a 95\% confidence interval of .67-.93, and I am justified in using this finding in my own work.  

We have three responses to this appeal to disciplinary conventions.  First,  disciplinary conventions are only justified insofar as they are indeed designed and followed correctly.  A number of statisticians and methodologists have criticized what Gigerenzer calls the "mindless" use of p-values and textbook statistical tests *[cite].  Insofar as these techniques are used without considering whether their assumptions are approximately satisfied, they can be much less epistemically trustworthy than their formal statistical properties indicate.  More fundamentally, the conventions themselves can be misdesigned.  For example, in psychology, biomedical research, and other fields, "statistical significance" (strictly, a p-value calculation of less than .05) is required for publication; studies with only "null results" effectively cannot be published *[cite].  This conventional publication standard produces substantial bias (in the statistical sense), which in part has contributed to the replication crisis in these fields *[Romero 2016].  

Second, these disciplinary standards may be laden with objectionable values.  For example, racist, sexist, classist, and ableist assumptions were common in medical, behavioral, and social sciences from the development of these fields as disciplines in the nineteenth century until recently (if not still today) *[cites].  Disciplinary conventions with morally objectionable assumptions are also untrustworthy.  

Third, the thought that disciplinary conventions can support shared interpretations is difficult to reconcile with the fact of disciplinary pluralism.  Different disciplines have different conventions; these differences can lead to misinterpretations or even incompatible bodies of evidence *[epistemic depth].  Patterns of misinterpretations and "epistemic depth" *[cite again] will tend to undermine trust, not bolster it.  


